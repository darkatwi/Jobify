{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Jobify — SkillNer Pretrained Model\n",
        "\n",
        "- This notebook contains everything needed to run skill extraction using the SkillNER pretrained model:\n",
        "\n",
        "1. Install dependencies (Transformers, Torch, pdfplumber, pypandoc + Pandoc).\n",
        "2. Import libraries needed for NLP + file handling.\n",
        "3. Convert input file to text (PDF / DOCX / TXT) + clean the text.\n",
        "4. Load the SkillNER pretrained model (ihk/skillner) and tokenizer.\n",
        "5. Build the token-classification pipeline (with aggregation to merge subwords/spans).\n",
        "6. Run skill extraction on the cleaned text and display the extracted skills + confidence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb0eb7a0",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "## 1. Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0bc6cad",
      "metadata": {
        "vscode": {
          "languageId": "powershell"
        }
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers pdfplumber pypandoc\n",
        "!apt-get -qq update\n",
        "!apt-get -qq install -y pandoc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85710b69",
      "metadata": {},
      "source": [
        "## 2. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb96e4e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pdfplumber\n",
        "import pypandoc\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f80cf455",
      "metadata": {},
      "source": [
        "## 3. Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af43d5d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# clean extracted text from docx and pdfs\n",
        "def clean_cv_text(text):\n",
        "    # Convert all text to lowercase\n",
        "    text = text.lower()  \n",
        "\n",
        "    # Replace multiple consecutive newlines with a single newline\n",
        "    text = re.sub(r'\\n+', '\\n', text)  \n",
        "    \n",
        "    # Replace multiple spaces, tabs, or newlines with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)  \n",
        "\n",
        "    # Remove page number artifacts commonly found in CV footers\n",
        "    text = re.sub(r'page \\d+ of \\d+', '', text)  \n",
        "\n",
        "    # Remove bullet characters and long dashes\n",
        "    text = re.sub(r'[•●▪■–—]', ' ', text)  \n",
        "\n",
        "    # Remove leading and trailing whitespace before sending text to the ML model\n",
        "    return text.strip()  \n",
        "\n",
        "\n",
        "\n",
        "# transform input from docx to text\n",
        "def docx_to_text(filepath):\n",
        "    text = pypandoc.convert_file(filepath, to='plain')\n",
        "    return text\n",
        "\n",
        "# transform input from pdf to text\n",
        "def pdf_to_text(filepath):\n",
        "    text = \"\"\n",
        "\n",
        "    with pdfplumber.open(filepath) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "\n",
        "    return text\n",
        "\n",
        "# exctract text from input (docx/pdf/txt)\n",
        "def extract_text(filepath):\n",
        "    type = os.path.splitext(filepath)[1].lower()\n",
        "\n",
        "    if type=='.docx':\n",
        "        return clean_cv_text(docx_to_text(filepath))\n",
        "    \n",
        "    elif type=='.pdf':\n",
        "        return clean_cv_text(pdf_to_text(filepath))\n",
        "    \n",
        "    elif type=='.txt':\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            return clean_cv_text(f.read())\n",
        "    \n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported '{type}' file type.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e02a3fe6",
      "metadata": {},
      "source": [
        "## 4. Model Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02ce8ced",
      "metadata": {},
      "outputs": [],
      "source": [
        "# load model\n",
        "def load_skillner():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"ihk/skillner\")\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\"ihk/skillner\")\n",
        "\n",
        "    return {'tokenizer': tokenizer, 'model': model}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b26a9026",
      "metadata": {},
      "source": [
        "## 5. Skills Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80298c02",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exctract skills from input (input text -> model -> skills)\n",
        "def build_skill_pipeline(model, tokenizer):\n",
        "    ner = pipeline(\n",
        "        task=\"token-classification\",\n",
        "        model=model,\n",
        "        tokenizer = tokenizer,\n",
        "        aggregation_strategy=\"simple\"   # merges B/I tokens\n",
        "    )\n",
        "\n",
        "    return ner\n",
        "\n",
        "def extract_skills(input, pipeline):\n",
        "    return pipeline(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12bc7505",
      "metadata": {},
      "source": [
        "## 6. Main Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d681cfcb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the skillner model and tokenizer once (run once)\n",
        "loaded = load_skillner()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "467c6608",
      "metadata": {},
      "outputs": [],
      "source": [
        "# build skills extraction pipeline once (run once)\n",
        "skillner = loaded['model']\n",
        "tokenizer = loaded['tokenizer']\n",
        "\n",
        "ner = build_skill_pipeline(model=skillner, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91456fa2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_main_pipeline(filepath, userID, userType):\n",
        "    # Convert file to text for input\n",
        "    input = extract_text(filepath=filepath)\n",
        "\n",
        "    # Extract skills from input\n",
        "    extracted_skills = extract_skills(input, ner)\n",
        "\n",
        "    # check if the user is a JobSeeker or a Company\n",
        "    if userType == 'company':\n",
        "        isJobSeeker = False\n",
        "    else:\n",
        "        isJobSeeker = True\n",
        "\n",
        "    # send skills to db\n",
        "    # send_to_db(extracted_skills, userID, isJobSeeker=isJobSeeker)\n",
        "\n",
        "    # print skills extracted\n",
        "    for i, skill in enumerate(extracted_skills):\n",
        "        print(f\"{i}. skill: {skill['word']}, confidence: {skill['score']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0658f5fa",
      "metadata": {},
      "source": [
        "## 7. Run Main Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba185722",
      "metadata": {},
      "outputs": [],
      "source": [
        "filepath = 'PUT FILEPATH TO THE CV HERE (DOCX/PDF/TXT)'\n",
        "\n",
        "results = run_main_pipeline(\n",
        "    filepath=filepath,\n",
        "    userID= 000000,\n",
        "    userType=\"admin\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a43927",
      "metadata": {},
      "source": [
        "## 8. Quick Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d49b002c",
      "metadata": {},
      "outputs": [],
      "source": [
        "test = pipeline(\n",
        "    \"token-classification\",\n",
        "    model=skillner,\n",
        "    tokenizer=tokenizer,\n",
        "    aggregation_strategy=\"simple\"\n",
        ")\n",
        "\n",
        "text = \"Experienced with Python, SQL, Docker, FastAPI, and React.\"\n",
        "test(text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Jobify_Colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
