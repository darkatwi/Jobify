{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eedb511c",
   "metadata": {},
   "source": [
    "# Jobify — Fine-Tuning JobBERT for Skill Extraction (NER)\n",
    "\n",
    "This notebook fine-tunes **jjzha/jobbert-base-cased** on **SkillSpan** for BIO skill tagging.\n",
    "\n",
    "Steps:\n",
    "1. Install dependencies\n",
    "2. Define data loader (SkillSpan splits)\n",
    "3. Define tokenizer+label alignment (BIO + masking)\n",
    "4. Build tf.data datasets\n",
    "5. Train JobBERT token classifier\n",
    "6. Evaluate using seqeval F1\n",
    "7. Save the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7476e0",
   "metadata": {},
   "source": [
    "## 2. Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e232713",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install transformers tensorflow datasets seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15255d12",
   "metadata": {},
   "source": [
    "## 3. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdc799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, TFAutoModelForTokenClassification, pipeline\n",
    "from seqeval.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e9ee18",
   "metadata": {},
   "source": [
    "## 4. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7235715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load skillspan dataset with official splits (train / validation / test)\n",
    "def load_skillspan_data():\n",
    "    splitted_data = load_dataset(\"jjzha/skillspan\")\n",
    "    data = concatenate_datasets([splitted_data['train'], splitted_data['validation'], splitted_data['test']])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = load_skillspan_data()\n",
    "\n",
    "train_X = data['train']['tokens']\n",
    "train_Y = data['train']['tags_skill']\n",
    "\n",
    "val_X = data['validation']['tokens']\n",
    "val_Y= data['validation']['tags_skill']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5260f2",
   "metadata": {},
   "source": [
    "## 5. Labels Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc8b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define labels\n",
    "label_list = [\"O\", \"B-SKILL\", \"I-SKILL\"]\n",
    "\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fd9d9e",
   "metadata": {},
   "source": [
    "## 6. Load Base-Cased Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a93037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jjzha/jobbert-base-cased\")\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\"jjzha/jobbert-base-cased\",\n",
    "                                                          num_labels=len(label_list),\n",
    "                                                          id2label=id2label,\n",
    "                                                          label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffdace6",
   "metadata": {},
   "source": [
    "## 7. Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize tokens (word-level) and align BIO labels to subword tokens\n",
    "# Uses -100 for tokens we want to ignore (special tokens + extra subwords)\n",
    "\n",
    "def tokenize_and_align_labels(tokenizer, tokens, tags):\n",
    "    # tokenize the data\n",
    "    tokenized = tokenizer(tokens, truncation=True, is_split_into_words=True, return_attention_mask=True, max_length=256)\n",
    "    \n",
    "    word_ids = tokenized.word_ids() # For each token produced by the tokenizer -> original word that it came from\n",
    "\n",
    "    labels = []\n",
    "    mask = []\n",
    "    prev_word_id = None\n",
    "\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            labels.append(0)\n",
    "            mask.append(0)\n",
    "\n",
    "        elif word_id != prev_word_id:\n",
    "            labels.append(tags[word_id])\n",
    "            mask.append(1)\n",
    "\n",
    "        else:\n",
    "            labels.append(0)\n",
    "            mask.append(0)\n",
    "\n",
    "        prev_word_id = word_id\n",
    "    \n",
    "    tokenized['labels'] = labels\n",
    "    tokenized['label_mask'] = mask\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3879ef",
   "metadata": {},
   "source": [
    "## 8. Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf12db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode data\n",
    "train_encoded = [\n",
    "    tokenize_and_align_labels(tokenizer, tokens, tags)\n",
    "    for tokens, tags in zip(train_X, train_Y)\n",
    "]\n",
    "\n",
    "val_encoded = [\n",
    "    tokenize_and_align_labels(tokenizer, tokens, tags)\n",
    "    for tokens, tags in zip(val_X, val_Y)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a8470c",
   "metadata": {},
   "source": [
    "## 9. DataSet Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdff4e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(encodings, batch_size=16):\n",
    "    input_ids = [e[\"input_ids\"] for e in encodings]\n",
    "    attention = [e[\"attention_mask\"] for e in encodings]\n",
    "    labels = [e[\"labels\"] for e in encodings]\n",
    "    weights = [e[\"label_mask\"] for e in encodings]\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(({\"input_ids\": input_ids, \"attention_mask\": attention}, labels, weights))\n",
    "\n",
    "    return ds.padded_batch(\n",
    "        batch_size,\n",
    "        padded_shapes=({\"input_ids\": [None], \"attention_mask\": [None]}, [None], [None]),\n",
    "        padding_values=({\"input_ids\": 0, \"attention_mask\": 0}, 0, 0.0)\n",
    "    ).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd778a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = make_dataset(train_encoded)\n",
    "val_data = make_dataset(val_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38737ec0",
   "metadata": {},
   "source": [
    "## 10. Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ac0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\"ML\\\\src\\\\models\\\\trained_models\\\\checkpoints\\\\jobify_jobbert_v1.keras\",\n",
    "                                                    monitor='val_loss',\n",
    "                                                    save_best_only=True,\n",
    "                                                    save_weights_only=False,\n",
    "                                                    mode='min',\n",
    "                                                    verbose=1)\n",
    "\n",
    "stop_training = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min', verbose=1, restore_best_weights=True)\n",
    "\n",
    "reduce_lrate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, verbose=1, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed8e46",
   "metadata": {},
   "source": [
    "## 11. Compiling and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba567220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile and train the model\n",
    "print(\"STARTING COMPILING PROCESS:\")\n",
    "model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=5e-05, epsilon=1e-08, beta_1=0.9, beta_2=0.999),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[\n",
    "                  tf.keras.metrics.Accuracy(),\n",
    "                  tf.keras.metrics.Precision(),\n",
    "                  tf.keras.metrics.Recall()])\n",
    "print(\"DONE COMPILING ✅\")\n",
    "\n",
    "print(\"STARTING TRAINING PROCESS: \")\n",
    "history = model.fit(train_data, validation_data=val_data, epochs=30, verbose=1, callbacks=[checkpoint, reduce_lrate, stop_training])\n",
    "print(\"DONE TRAINING ✅\")\n",
    "np.savez(\"ML\\\\src\\\\models\\\\training_history\\\\training_history.npz\", **history.history)\n",
    "print(\"HISTORY SAVED ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fcd32e",
   "metadata": {},
   "source": [
    "## 12. Evaluation (F1-SCORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert model outputs and true labels into seqeval-compatible format\n",
    "def get_seqeval_input(model, dataset, id2label):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in dataset:\n",
    "        input = batch[0]\n",
    "        true_labels = batch[1]\n",
    "        weights = batch[2]\n",
    "\n",
    "        logits = model(input, training=False).logits\n",
    "        pred_ids = tf.argmax(logits, axis=-1).numpy()\n",
    "        label_ids = true_labels.numpy()\n",
    "        weights = weights.numpy()\n",
    "\n",
    "        for i in range(pred_ids.shape[0]):\n",
    "            preds, labels = [], []\n",
    "            \n",
    "            for j in range(pred_ids.shape[1]):\n",
    "\n",
    "                if weights[i, j] == 0:\n",
    "                    continue  # ignore masked tokens\n",
    "\n",
    "                preds.append(id2label[int(pred_ids[i, j])])\n",
    "                labels.append(id2label[int(label_ids[i, j])])\n",
    "\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels)\n",
    "            \n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "\n",
    "\n",
    "# evaluate the NER model\n",
    "def evaluate_ner(model, dataset, id2label):\n",
    "    y_pred, y_true = get_seqeval_input(model, dataset, id2label)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96717fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = evaluate_ner(model, val_data, id2label)\n",
    "print(\"Validation seqeval F1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f72079",
   "metadata": {},
   "source": [
    "## 12. Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e878140",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"\"\n",
    "\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "print(\"Saved ✅ to:\", SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de94da87",
   "metadata": {},
   "source": [
    "## 13. Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af07f39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"\"\n",
    "\n",
    "jobify_tokenizer = AutoTokenizer.from_pretrained(SAVE_DIR)\n",
    "jobify_model = TFAutoModelForTokenClassification.from_pretrained(SAVE_DIR)\n",
    "\n",
    "print(\"Model loaded ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712b80c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=jobify_model,\n",
    "    tokenizer=jobify_tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "text = \"Experienced with Python, SQL, Docker, FastAPI, and React.\"\n",
    "test(text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
